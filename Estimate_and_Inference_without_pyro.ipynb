{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hellocybernetics/Pyro_tutorials/blob/master/Estimate_and_Inference_without_pyro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hA-qlJZ9Nw4v",
        "colab_type": "code",
        "outputId": "fe84dc28-9656-4503-8075-6b2599f82369",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pyro-ppl"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyro-ppl in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: graphviz>=0.8 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.10.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.11.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.0.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (2.2)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.14.6)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.5.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (2.3.2)\n",
            "Requirement already satisfied: tqdm>=4.28 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (4.28.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.2->pyro-ppl) (4.3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "459cvjGfOGyN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pyroの基本的な役割\n",
        "PyroはPyTorchを計算のバックエンドに構えた確率プログラミング言語です。確率プログラミング言語の主な役割は、様々な確率分布からのサンプリングや、同時分布・条件付き分布・周辺分布の取扱を容易にすることです。**まずはTorchを使ってみて、その確率プログラミング言語の必要性を体感してみましょう。**"
      ]
    },
    {
      "metadata": {
        "id": "77PKI6F3N6Ec",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.distributions as torchdist\n",
        "\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "import pyro.poutine as poutine"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LSELqDR0PTtu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 分布の記述\n",
        "まず、肩慣らしに標準正規分布 $\\rm {Normal} (0, 1)$ を書いてみましょう。`torch.distributions`  モジュールを使うことで下記のように記述することができます。"
      ]
    },
    {
      "metadata": {
        "id": "bPIXw1EnPDU2",
        "colab_type": "code",
        "outputId": "c3ea5a6d-e2af-44da-d0f6-9d8b9d5a237d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "normal_dist = torchdist.Normal(loc=torch.tensor(0.), scale=torch.tensor(1.))\n",
        "normal_dist"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Normal(loc: 0.0, scale: 1.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "k93zrklnPuis",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### サンプリング\n",
        "続いて、記述した分布から\n",
        "$$\n",
        "x \\sim {\\rm Normal}(0, 1)\n",
        "$$\n",
        "とサンプリングを得るには下記のようにします。"
      ]
    },
    {
      "metadata": {
        "id": "9Apgnf6sQFe0",
        "colab_type": "code",
        "outputId": "bc61dc50-8ef2-471c-fde0-efc493bcbb45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "x = normal_dist.sample()\n",
        "x"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-1.2863)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "mq-6TcUIQPqS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "更に同じ分布から独立に複数のサンプルを得たい場合には下記のように、`sample()` メソッドに引数を渡すことで実施します。このとき引数はリストあるいはタプルで渡すようにします。"
      ]
    },
    {
      "metadata": {
        "id": "kc0-utrYQnVK",
        "colab_type": "code",
        "outputId": "075ce699-4010-4d1b-b55a-658d7a8578ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "X1 = normal_dist.sample([3,])\n",
        "X1"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.2014, -0.4246, -0.7346])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "Puq8R9cqRihc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "上記のサンプルは各成分が各々独立に正規分布から生成されています。数式で表現するならば下記のようになります。\n",
        "\n",
        "$$\n",
        "X_1[i] \\sim \\rm{Normal}(0, 1)\n",
        "$$\n",
        "\n",
        "同じ理屈でもっと多次元の配列を作ることもできます。"
      ]
    },
    {
      "metadata": {
        "id": "VzE_FNBCQqKa",
        "colab_type": "code",
        "outputId": "d1856aae-5ab2-4820-f005-0e810e2166e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "cell_type": "code",
      "source": [
        "X2 = normal_dist.sample([10, 2])\n",
        "X2"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1922, -1.7830],\n",
              "        [-1.1470, -0.4452],\n",
              "        [ 2.4465, -0.0318],\n",
              "        [-0.6488, -0.7740],\n",
              "        [ 1.1140, -2.6986],\n",
              "        [-2.4984,  0.5078],\n",
              "        [-0.1208, -0.7932],\n",
              "        [-0.4452, -0.1742],\n",
              "        [ 0.8060,  0.1316],\n",
              "        [ 0.8556, -0.2976]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "yfcsEdNdRvic",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "上記の配列も同様に各成分が独立に正規分布から生成されています。\n",
        "\n",
        "$$\n",
        "X_2[i, j] = \\rm{Normal}(0, 1)\n",
        "$$\n",
        "\n",
        "配列として取り出すと、もはや元々これが何の分布の話だったのか、情報を持ち合わせていないので注意が必要です。例えば、下記のような2次元正規分布を考えましょう。2次元正規分布からの1つのサンプルは当然2つの要素を持っています。"
      ]
    },
    {
      "metadata": {
        "id": "K5GrSGa7RBAK",
        "colab_type": "code",
        "outputId": "8e390e93-1401-4e8e-dc62-3f20fb261d12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "multi_normal_dist = torchdist.MultivariateNormal(\n",
        "    loc=torch.tensor([1., -2.]),\n",
        "    covariance_matrix=torch.tensor([[1., -1.],\n",
        "                                    [-1., 2.]])\n",
        ")\n",
        "\n",
        "multi_normal_dist.sample()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.2737, -1.9357])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "8gKtZFYpVB0C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "この2次元正規分布から10個のサンプルを得ると、`(10, 2)` の `tensor`が得られますが、この `tensor` の各成分は独立ではありません。各行は独立ですが、各列は2次元の正規分布からの一つのサンプルであり、相関を持っていることに注意する必要があります。\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "M1BGa6oXUILa",
        "colab_type": "code",
        "outputId": "50306727-5cf2-4669-b3d2-699428e83d5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "cell_type": "code",
      "source": [
        "multi_normal_dist.sample([10])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.3732, -0.6946],\n",
              "        [ 0.6025, -1.6951],\n",
              "        [-0.0659, -0.3871],\n",
              "        [-0.6826,  0.8660],\n",
              "        [ 1.1101, -0.2939],\n",
              "        [ 1.7970, -3.4589],\n",
              "        [ 0.2036,  0.9662],\n",
              "        [ 1.4677, -2.8118],\n",
              "        [ 1.0790, -1.5148],\n",
              "        [ 1.2286, -2.5608]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "sHtUrjA7VszA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "長いコードを書いているうちに、ある`tensor` がどういうサンプルのされ方をしたものであったのかを見失うことも起こりうるので注意が必要です。"
      ]
    },
    {
      "metadata": {
        "id": "ED47SK9QVuBv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 尤度の計算\n",
        "機械学習ではしばしば確率分布 $p(x)$ の尤度の計算が必要になります。あるサンプル $x^*$ の $p(x)$ における尤度は単純に $p(x^*)$ の値になります。では $\\{x_1, x_2, x_3,...,\\}$ の尤度はどのように計算するのかというと、\n",
        "\n",
        "$$\n",
        "p(x_1)p(x_2)p(x_3)...\n",
        "$$\n",
        "\n",
        "と各確率値の積によって表されます。確率値は $[0, 1]$ の値でありサンプルの数が多くなると、$1$ 未満の数の積が連なり非常に小さな値となってしまいます。そこで、コンピュータでは通常、対数尤度というものを計算することにします。\n",
        "\n",
        "$$\n",
        "{\\rm log}\\{p(x_1)p(x_2)p(x_3)...\\} = {\\rm log}p(x_1) + {\\rm log}p(x_2) + {\\rm log}p(x_3) + ...\n",
        "$$\n",
        "\n",
        "と負の値の足し算になり数値的に安定します。また、$\\rm log$ は単調増加関数であるので、微分にとっても符号には影響が無く（絶対値には影響するが）、積が和となっているため都合が良いです。サンプル $x^*$ の対数尤度は簡単に下記のコードで求まります。"
      ]
    },
    {
      "metadata": {
        "id": "1LnrmXtHWQs8",
        "colab_type": "code",
        "outputId": "cfaa93a6-4750-49fb-fce6-3a3768c48fea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "sample = multi_normal_dist.sample()\n",
        "print(\"*** a sample from 2dim normal: \\n\", sample)\n",
        "log_likelihood = multi_normal_dist.log_prob(sample)\n",
        "print(\"*** log likelihood of the sample \\n\", log_likelihood)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** a sample from 2dim normal: \n",
            " tensor([ 0.4844, -1.2218])\n",
            "*** log likelihood of the sample \n",
            " tensor(-2.0053)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qZb5n79oWebI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "また複数のサンプルに関しては足し算する前の各々の対数尤度が返ってきます。"
      ]
    },
    {
      "metadata": {
        "id": "KRHth-wsYfF-",
        "colab_type": "code",
        "outputId": "4ccc7662-10e0-472a-a877-ff03a595d2d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "samples = multi_normal_dist.sample([5,])\n",
        "print(\"*** 5 samples from 2dim normal: \\n\", sample)\n",
        "log_likelihoods = multi_normal_dist.log_prob(samples)\n",
        "print(\"*** log likelihoods of the samples: \\n\", log_likelihoods)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** 5 samples from 2dim normal: \n",
            " tensor([ 0.4844, -1.2218])\n",
            "*** log likelihoods of the samples: \n",
            " tensor([-1.8611, -2.2424, -5.2715, -3.8392, -2.3916])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wy4ONNQrYkgi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### MAP推定\n",
        "ここでMAP推定を実施してみます。上記の関数と自動微分機能を駆使すれば難しくありません。\n",
        "\n",
        "#### 訓練データ\n",
        "人工訓練データを作りますが、パラメータは知らない体で推定をします。"
      ]
    },
    {
      "metadata": {
        "id": "M1mu_XCBZ-8e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def toy_data():\n",
        "    true_mu = torch.tensor([2., 3.])\n",
        "    true_cov = torch.tensor([[2., -3.],[-3, 5]])\n",
        "    \n",
        "    X = torchdist.MultivariateNormal(loc=true_mu, \n",
        "                                     covariance_matrix=true_cov).sample([100,])\n",
        "    return X\n",
        "\n",
        "X_train = toy_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RePK_G1Kac-j",
        "colab_type": "code",
        "outputId": "7548c782-fe06-4c13-87a2-87debfc6e52a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(X_train.numpy()[:,0], X_train.numpy()[:,1], \"o\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f80f3996320>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAFKCAYAAABcq1WoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAQAw7P8DE/ZTUadEP9MAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "2C3CoLtMbFP-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 推定のためのモデル\n",
        "データを可視化して、コレは2次元正規分布に従っていると睨んだとしましょう。すると私達が推定したいパラメータは、平均ベクトルと共分散行列になります。これらをまず初期化しておきます。"
      ]
    },
    {
      "metadata": {
        "id": "O5WouLd4beCO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mu_param = torch.nn.Parameter(torch.tensor([0., 0.]))\n",
        "cov_tril_param = torch.nn.Parameter(torch.tensor([[1., 0.],\n",
        "                                                  [0., 1.]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IdOxDc-Nc0gP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 対数尤度関数（最適化の目的関数）\n",
        "データ $X = \\{x_1, \\cdots, x_N\\}$ に対する対数尤度関数は\n",
        "\n",
        "$$\n",
        "{\\rm LogLikelihood}(X, \\mu, \\Sigma) = \\frac{1}{N}\\sum_{i=1}^N {\\rm log}\\{{\\rm Normal}(x_i \\mid \\mu, \\Sigma)\\}\n",
        "$$\n",
        "\n",
        "であり、データ $X$ は既に手元にあるので、パラメータ $\\mu, \\Sigma$ に関して最大化するのが最尤推定法になります。続いて事前分布 $p(\\mu)$ と $p(\\Sigma)$ を目的関数に考慮するのがMAP推定になります。具体的には下記のように修正されます。\n",
        "\n",
        "$$\n",
        "{\\rm LogJointProb}(X, \\mu, \\Sigma) = \\frac{1}{N}\\sum_{i=1}^N {\\rm log}\\{{\\rm Normal}(x_i \\mid \\mu, \\Sigma)\\} + {\\rm log}p(\\mu) +{\\rm log}p(\\Sigma)\n",
        "$$\n",
        "\n",
        "今回は適当に分散の大きな正規分布を仮定し、無情報事前分布に近い状態にしておきます。\n",
        "\n",
        "#### 補足\n",
        "ちなみにベイズの定理よりパラメータの事後分布は\n",
        "\n",
        "$$\n",
        "p(\\mu, \\Sigma \\mid X) = \\frac{p(X, \\mu, \\Sigma)}{p(X)}\n",
        "$$\n",
        "\n",
        "と表され、この分母は定数になっています。したがって分子の同時分布について最大化すれば良いというのかMAP推定であり、同時分布を\n",
        "\n",
        "$$\n",
        "p(X,\\mu,\\Sigma)=p(X\\mid \\mu, \\Sigma) p(\\mu) p(\\Sigma)\n",
        "$$\n",
        "\n",
        "と表現しつつ、対数を取ることで上記の目的関数が導出されます。"
      ]
    },
    {
      "metadata": {
        "id": "Iya1lNR9b2Eu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def log_joint_prob(mu_param, cov_param, X):\n",
        "    \n",
        "    prior_mu = torchdist.Normal(loc=torch.zeros_like(mu_param),\n",
        "                                 scale=100*torch.ones_like(mu_param))\n",
        "    prior_cov = torchdist.Normal(loc=torch.zeros_like(cov_param),\n",
        "                                 scale=100*torch.ones_like(cov_param))\n",
        "    \n",
        "    model = torchdist.MultivariateNormal(\n",
        "        loc=mu_param, \n",
        "        scale_tril=cov_param\n",
        "    )\n",
        "    \n",
        "    return (\n",
        "        model.log_prob(X).mean() \n",
        "        + prior_mu.log_prob(mu_param).mean()\n",
        "        + prior_cov.log_prob(cov_param).mean()\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mkk01MKRd2UZ",
        "colab_type": "code",
        "outputId": "7947a72a-852e-4aff-c845-eef8830b5097",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"*** initial log joint prob\")\n",
        "log_joint_prob(mu_param, cov_tril_param, X_train)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** initial log joint prob\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-22.1863, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "_bOHbf89kWuo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=[mu_param, cov_tril_param], lr=1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FyyuYqyEkw3b",
        "colab_type": "code",
        "outputId": "ae97c93f-3819-48fa-8a6a-7629b9ee154e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(10000):\n",
        "    optimizer.zero_grad()\n",
        "    log_joint_prob_value = log_joint_prob(mu_param, cov_tril_param, X_train)\n",
        "    loss_value = - log_joint_prob_value\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (i+1) % 1000 == 0 or (i==0):\n",
        "        print(loss_value.detach().numpy())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22.186283\n",
            "16.31262\n",
            "15.428556\n",
            "14.82558\n",
            "14.366702\n",
            "13.900938\n",
            "13.686693\n",
            "13.684849\n",
            "13.684792\n",
            "13.684792\n",
            "13.684792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w0AAZ-oPlakF",
        "colab_type": "code",
        "outputId": "e60b42a4-bea5-461b-a56b-2ca7ddefed3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def toy_data():\n",
        "    true_mu = torch.tensor([2., 3.])\n",
        "    true_cov = torch.tensor([[2., -3.],\n",
        "                             [-3., 5.]])\n",
        "    \n",
        "    X = torchdist.MultivariateNormal(loc=true_mu, \n",
        "                                     covariance_matrix=true_cov).sample([100,])\n",
        "    return X\n",
        "\"\"\"\n",
        "\n",
        "print(\"mu map estimated: \\n\", mu_param.data)\n",
        "print(\"cov map estimated \\n\", cov_tril_param.mm(cov_tril_param.t()).data)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mu map estimated: \n",
            " tensor([1.9917, 3.0081])\n",
            "cov map estimated \n",
            " tensor([[ 1.5979, -2.3872],\n",
            "        [-2.3872,  3.9845]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5qLM4A32m_Ko",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 回帰モデル\n",
        "次に同じようにTorchで回帰モデルを実装していきます。スカラーの入力 $x$ とスカラーの出力 $y$ の関係性が下記の用に多項式で表されると仮定しましょう。\n",
        "\n",
        "$$\n",
        "y = -3 + 4x + x^2 + \\epsilon\n",
        "$$\n",
        "\n",
        "ただし、ここで $\\epsilon \\sim {\\rm Normal}(0, 1)$ の正規乱数としておきます。"
      ]
    },
    {
      "metadata": {
        "id": "oRiDD1Ux1vP4",
        "colab_type": "code",
        "outputId": "74136081-d791-4cfb-d573-b9c4cd84dabe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "cell_type": "code",
      "source": [
        "def toy_poly():\n",
        "    \n",
        "    x = 5 * torch.rand(100, 1) \n",
        "    linear_op = -3 - 4*x + 1*x**2 \n",
        "    y = torchdist.Normal(linear_op, 1).sample()\n",
        "    return x, y\n",
        "\n",
        "x_train, y_train = toy_poly()\n",
        "\n",
        "plt.plot(x_train.numpy(), y_train.numpy(), \"o\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f80f39806a0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAFKCAYAAABcq1WoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4QsAgGWELwAAlhG+AABY9v8BqAY4IVvdSzEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "G4wncEBU2n3x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### モデル\n",
        "さて、上記のデータを見て、二次関数で表現できると睨んだとしましょう（出来レースですが）。\n",
        "\n",
        "$$\n",
        "y = w_0 + w_1x + w_2x^2 + \\epsilon\n",
        "$$\n",
        "\n",
        "すると、簡単な式変形から下記のような確率モデルを使うことが考えられます（$\\epsilon$の分散は既知としているが、未知にすることも当然必要であれば考えられる）。 \n",
        "\n",
        "\n",
        "$$\n",
        "y - (w_0 + w_1x + w_2x^2) \\sim {\\rm Normal}(0,1)\n",
        "$$\n",
        "\n",
        "あるいは、分布の平均の方にパラメータを持つ項を吸収させて\n",
        "\n",
        "$$\n",
        "y \\sim {\\rm Normal}(w_0 + w_1x + w_2x^2 ,1)\n",
        "$$\n",
        "\n",
        "とできます。事前分布を適当に置いてしまえば、MAP推定に必要な同時分布 $p(w_0, w_1, w_2, x, y)$ が一先ず書き下せますので、その対数値を最大化することで回帰問題を解くことが可能になります。今回も各パラメータに対してはそれぞれ適当な正規分布を仮定してしまいましょう。\n",
        "\n",
        "$$\n",
        "{\\rm LogJointProb}(w_0, w_1, w_2, X, Y) = \\frac{1}{N} \\sum_{i=1}^N {\\rm Normal}(y_i\\mid w_0 + w_1x_i + w_2x_i^2 ,1) +{\\rm log} p(w_0) +{\\rm log} p(w_1) +{\\rm log} p(w_2)\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "id": "LmDgr1854-ZS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w0 = torch.nn.Parameter(torch.tensor(1.))\n",
        "w1 = torch.nn.Parameter(torch.tensor(1.))\n",
        "w2 = torch.nn.Parameter(torch.tensor(1.))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yM3nPHcf5J25",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def log_joint_prob(w0, w1, w2, x, y):\n",
        "    \n",
        "    prior_w0 = torchdist.Normal(torch.tensor(0.), 10*torch.tensor(1.))\n",
        "    prior_w1 = torchdist.Normal(torch.tensor(0.), 10*torch.tensor(1.))\n",
        "    prior_w2 = torchdist.Normal(torch.tensor(0.), 10*torch.tensor(1.))\n",
        "\n",
        "    linear = w0 + w1*x + w2*x**2\n",
        "    likelihood = torchdist.Normal(linear, torch.ones_like(linear))\n",
        "    \n",
        "    return (\n",
        "        prior_w0.log_prob(w0).mean() +\n",
        "        prior_w1.log_prob(w1).mean() +\n",
        "        prior_w2.log_prob(w2).mean() +\n",
        "        likelihood.log_prob(y).mean()\n",
        "    )    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DcnPsxO-T9Ie",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "1ad91adf-38a2-4a62-fbc2-07a20ae839cd"
      },
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=[w0, w1, w2], lr=1e-3)\n",
        "\n",
        "for i in range(30000):\n",
        "    optimizer.zero_grad()\n",
        "    log_joint_prob_value = log_joint_prob(w0, w1, w2, x_train, y_train)\n",
        "    loss_value = - log_joint_prob_value\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (i+1) % 1000 == 0 or (i==0):\n",
        "        print(loss_value.detach().numpy())"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "158.7254\n",
            "38.026344\n",
            "20.888271\n",
            "18.741535\n",
            "16.68861\n",
            "14.654636\n",
            "12.988394\n",
            "11.903213\n",
            "11.39063\n",
            "11.242154\n",
            "11.214649\n",
            "11.199949\n",
            "11.185936\n",
            "11.175887\n",
            "11.171421\n",
            "11.17048\n",
            "11.170418\n",
            "11.170418\n",
            "11.170418\n",
            "11.170418\n",
            "11.170418\n",
            "11.170418\n",
            "11.170418\n",
            "11.170419\n",
            "11.170418\n",
            "11.170418\n",
            "11.170418\n",
            "11.170418\n",
            "11.170418\n",
            "11.170417\n",
            "11.170418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vfoligfvUrqV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "26fea036-5edd-44a3-a943-80edf6dfe2bb"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def toy_poly():\n",
        "    \n",
        "    x = 5 * torch.rand(100, 1) \n",
        "    linear_op = -3 - 4*x + 1*x**2 \n",
        "    y = torchdist.Normal(linear_op, 1).sample()\n",
        "    return x, y\n",
        "\n",
        "\"\"\"\n",
        "print(w0.data)\n",
        "print(w1.data)\n",
        "print(w2.data)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-3.1738)\n",
            "tensor(-3.5922)\n",
            "tensor(0.9117)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hh15BgdT-v3M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 変分推論\n",
        "ベイズ推論は事後分布 \n",
        "\n",
        "$$\n",
        "p(\\theta \\mid D) = \\frac{p(D\\mid \\theta)p(\\theta)}{p(D)}\n",
        "$$\n",
        "\n",
        "において、同時分布の最大化に甘んじず（すなわち関数の一番山となる部分だけを探すのではなく）、分布の形状全体を把握しようという試みになります。その試みは一般に困難を極めます。都合の良い尤度関数と都合の良い事前分布を選ばない限りは形状全体を上手に求めることはできません。\n",
        "\n",
        "したがって、形状全体を知りたいのだが、ある程度簡略化した形状で一番近いものを探せればいいというのが変分推論です。変分モデル $q(\\theta; \\eta)$ を仮定し（$\\eta$は変分パラメータと呼ばれる最適化すべきパラメータである）、$\\eta$ の調整で分布の形状を変えること $p(\\theta |D)$ に最も近い $q(\\theta; \\eta)$ を決定します。近いというのはKLダイバージェンスの意味であり\n",
        "\n",
        "$$\n",
        "{\\rm KL}[q(\\theta; \\eta) : p(\\theta\\mid D)] = {\\mathbb E}_{q(\\theta;\\eta)}[{\\rm log}q(\\theta; \\eta)] - {\\mathbb E}_{q(\\theta;\\eta)}[{\\rm log}p(\\theta)] - {\\mathbb E}_{q(\\theta;\\eta)}[{\\rm log}p(D\\mid\\theta)]\n",
        "$$\n",
        "\n",
        "を最小化するような $\\eta$ を求めます。\n",
        "\n",
        "こちらも最適化問題ではありますが、求めているものはパラメータ $\\theta $ の値ではなくパラメータ $\\theta$ が取りうる値の分布を網羅的に把握するために $\\eta$ を最適化していることに注意しましょう。最適化された $\\eta$ によって分布 $q(\\theta ; \\eta)$ が定まり、この分布からサンプリングをしたりすることで、単に点推定で $\\theta$ を決めてしまうよりも多くの情報を利用することができるというわけです。\n",
        "\n",
        "実際の推論では期待値計算の代わりに現在の $\\eta$ の値を用いて \n",
        "\n",
        "$$\n",
        "\\theta^* \\sim q(\\theta; \\eta)\n",
        "$$\n",
        "\n",
        "とサンプリングし、サンプリングされた$\\theta^*$ で現在のKLダイバージェンスを計算するということにします（そんなのいい加減すぎる！と思うのであれば、$q(\\theta; \\eta)$ は現在の $\\eta$ を使うとして重点サンプリングなどをしてもいいだろうし、大げさにもMCMCを使ってもいいだろう。単に計算量の問題である）。\n",
        "\n",
        "$$\n",
        "{\\rm KL}[q(\\theta; \\eta) : p(\\theta\\mid D)] \\simeq {\\rm log}q(\\theta^*; \\eta) - {\\rm log}p(\\theta^*) - {\\rm log}p(D\\mid\\theta^*)\n",
        "$$\n",
        "\n",
        "\n",
        "#### 変分モデル\n",
        "回帰問題の例に戻って、パラメータ $w_0, w_1, w_2$ に対してそれぞれ変分モデル\n",
        "\n",
        "$$\n",
        "q(w_i ; \\eta_i) = {\\rm Normal}(\\mu_i, \\sigma_i)\n",
        "$$\n",
        "\n",
        "を仮定しましょう。すなわち各 $w_i$ に対して正規分布を仮定して、あとはそれぞれの平均分散を変分パラメータとして最適化して $w_i$ の分布を得てしまおうということにしたのです。"
      ]
    },
    {
      "metadata": {
        "id": "n4xTF9EUCD-D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "variational_params = {\n",
        "    \"w0_loc\": torch.nn.Parameter(torch.tensor(0.)),\n",
        "    \"w0_scale_log\": torch.nn.Parameter(torch.tensor(0.)),\n",
        "    \"w1_loc\": torch.nn.Parameter(torch.tensor(0.)),\n",
        "    \"w1_scale_log\": torch.nn.Parameter(torch.tensor(0.)),\n",
        "    \"w2_loc\": torch.nn.Parameter(torch.tensor(0.)),\n",
        "    \"w2_scale_log\": torch.nn.Parameter(torch.tensor(0.)),\n",
        "}\n",
        "\n",
        "def variational_model(variational_params):\n",
        "    \"\"\"\n",
        "    Variational model q(w; eta)\n",
        "    arg: variational parameters \"eta\"\n",
        "    return: w ~ q(w; eta)\n",
        "    \"\"\"\n",
        "    w0_q = torchdist.Normal(\n",
        "        variational_params[\"w0_loc\"],\n",
        "        torch.exp(variational_params[\"w0_scale_log\"]),\n",
        "    )\n",
        "    \n",
        "    w1_q = torchdist.Normal(\n",
        "        variational_params[\"w1_loc\"],\n",
        "        torch.exp(variational_params[\"w1_scale_log\"]),\n",
        "    )\n",
        "    \n",
        "    w2_q = torchdist.Normal(\n",
        "        variational_params[\"w2_loc\"],\n",
        "        torch.exp(variational_params[\"w2_scale_log\"]),\n",
        "    )\n",
        "    \n",
        "    return w0_q, w1_q, w2_q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "huYHejDTJOSy",
        "colab_type": "code",
        "outputId": "b90c76dd-35d3-4fdf-f1ca-2944357cc769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"*** w0, w1, w2 :variational model with initial variatinal params\")\n",
        "variational_model(variational_params)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** w0, w1, w2 :variational model with initial variatinal params\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Normal(loc: 0.0, scale: 1.0),\n",
              " Normal(loc: 0.0, scale: 1.0),\n",
              " Normal(loc: 0.0, scale: 1.0))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "metadata": {
        "id": "UrU74P8QJdKv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### KLダイバージェンス\n",
        "変分モデルを書き終えたので、後は変分モデルから出てくるサンプルを引数としたKLダイバージェンスを書き下せばいいです。KLダイバージェンスの近似は実は下記の通り、MAP推定でも利用してきたLog Joint Prob（データ $D$とパラメータ $\\theta$の対数同時確率）\n",
        "と新たに出てきたVariational model （勝手に仮定したパラメータ $\\theta $ の事後分布）の対数確率値で構成されています。\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "{\\rm KL}[q(\\theta; \\eta) : p(\\theta\\mid D)] &\\simeq {\\rm log}q(\\theta^*; \\eta) - {\\rm log}p(\\theta^*) - {\\rm log}p(D\\mid\\theta^*) \\\\\\ \n",
        "& = {\\rm log}q(\\theta^*; \\eta) - {\\rm LogJointProb}(\\theta^*, D)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "ですので、実装上はMAP推定に使っていた目的関数をそのまま流用できます。\n",
        "\n",
        "すなわち実装は、$\\eta$ の関数である変分モデルを作り、パラメータ $\\theta$ をサンプリングできるようにして、そのサンプリング値$\\theta^*$をMAP推定で使っていた目的関数に代入してやれば良いのです。更に、サンプリングされた $\\theta^*$の$q(\\theta; \\eta)$ におけるの対数確率値も追加してやれば、目的関数の作成は終了です。"
      ]
    },
    {
      "metadata": {
        "id": "fkMzu9zJNNPp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def kl_divergence(variational_params, x, y):\n",
        "    w0_q, w1_q, w2_q = variational_model(variational_params)\n",
        "    \n",
        "    w0_sample = w0_q.sample()\n",
        "    w1_sample = w1_q.sample()    \n",
        "    w2_sample = w2_q.sample()\n",
        "    \n",
        "    log_joint_prob_value = log_joint_prob(w0_sample, w1_sample, w2_sample, x, y)\n",
        "    log_variational_prob_value = (\n",
        "        w0_q.log_prob(w0_sample) +\n",
        "        w1_q.log_prob(w1_sample) +\n",
        "        w2_q.log_prob(w2_sample)\n",
        "    )\n",
        "    \n",
        "    return log_variational_prob_value - log_joint_prob_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-eCSuQ4cP2ua",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "25dd7250-4705-4b1d-e0e5-9c4f77afdc2d"
      },
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(params=variational_params.values(), lr=1e-8)\n",
        "\n",
        "for i in range(9000):\n",
        "    optimizer.zero_grad()\n",
        "    loss_value =kl_divergence(variational_params, x_train, y_train)\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (i+1) % 300 == 0 or (i==0):\n",
        "        print(loss_value.detach().numpy())"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51.307236\n",
            "30.228342\n",
            "24.399588\n",
            "94.61427\n",
            "54.46742\n",
            "256.63162\n",
            "46.625977\n",
            "121.564\n",
            "83.602974\n",
            "36.613064\n",
            "38.916714\n",
            "52.55144\n",
            "46.949112\n",
            "29.521364\n",
            "66.69088\n",
            "175.45775\n",
            "31.18562\n",
            "46.020782\n",
            "128.05133\n",
            "28.738707\n",
            "21.411339\n",
            "122.938194\n",
            "56.800346\n",
            "67.371475\n",
            "24.99657\n",
            "57.510315\n",
            "18.435207\n",
            "26.601477\n",
            "33.188393\n",
            "34.200565\n",
            "61.512882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-z6gnrVnWD8C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "664b6f37-7983-44e4-9987-31d176704193"
      },
      "cell_type": "code",
      "source": [
        "for k, v in variational_params.items():\n",
        "    print(\"{} : {}\".format(k, v))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w0_loc : 1.2164202871645102e-06\n",
            "w0_scale_log : -6.172373900881212e-07\n",
            "w1_loc : -1.4097398661760963e-06\n",
            "w1_scale_log : 9.952343589247903e-07\n",
            "w2_loc : 1.3103788205626188e-06\n",
            "w2_scale_log : -2.068474259431241e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VBfVH_TZYnIS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 学習の仕方を変える\n",
        "さて、結果は上手く言ったと言えるのでしょうか…？どうやらイマイチな気がしますね。変分推論では、変分パラメータを同時に一気に最適化するのは難しいので、実際には変分パラメータを1つずつ更新していく必要があります。\n",
        "\n",
        "具体的には学習のコードを下記のように書き直します。まずは `optimizer` を各々のパラメータに対して設定し、個別にパラメータを1つずつ更新できるようにしておきましょう。"
      ]
    },
    {
      "metadata": {
        "id": "R77Q_0INddb2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "outputId": "7f7152a0-1dca-41c9-99cd-dd2a39551035"
      },
      "cell_type": "code",
      "source": [
        "optimizers = {}\n",
        "for key in variational_params.keys():\n",
        "    optimizers[key] = torch.optim.SGD(params=[variational_params[key]], lr=1e-6)\n",
        "\n",
        "optimizers"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'w0_loc': SGD (\n",
              " Parameter Group 0\n",
              "     dampening: 0\n",
              "     lr: 1e-06\n",
              "     momentum: 0\n",
              "     nesterov: False\n",
              "     weight_decay: 0\n",
              " ), 'w0_scale_log': SGD (\n",
              " Parameter Group 0\n",
              "     dampening: 0\n",
              "     lr: 1e-06\n",
              "     momentum: 0\n",
              "     nesterov: False\n",
              "     weight_decay: 0\n",
              " ), 'w1_loc': SGD (\n",
              " Parameter Group 0\n",
              "     dampening: 0\n",
              "     lr: 1e-06\n",
              "     momentum: 0\n",
              "     nesterov: False\n",
              "     weight_decay: 0\n",
              " ), 'w1_scale_log': SGD (\n",
              " Parameter Group 0\n",
              "     dampening: 0\n",
              "     lr: 1e-06\n",
              "     momentum: 0\n",
              "     nesterov: False\n",
              "     weight_decay: 0\n",
              " ), 'w2_loc': SGD (\n",
              " Parameter Group 0\n",
              "     dampening: 0\n",
              "     lr: 1e-06\n",
              "     momentum: 0\n",
              "     nesterov: False\n",
              "     weight_decay: 0\n",
              " ), 'w2_scale_log': SGD (\n",
              " Parameter Group 0\n",
              "     dampening: 0\n",
              "     lr: 1e-06\n",
              "     momentum: 0\n",
              "     nesterov: False\n",
              "     weight_decay: 0\n",
              " )}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "metadata": {
        "id": "v7kEd7N9dweL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "358062f6-5fe9-46ce-920f-08ae3d0956d3"
      },
      "cell_type": "code",
      "source": [
        "for i in range(9000):\n",
        "\n",
        "    def step(key):\n",
        "        optimizers[key].zero_grad()\n",
        "        loss_value =kl_divergence(variational_params, x_train, y_train)\n",
        "        loss_value.backward()\n",
        "        optimizers[key].step()\n",
        "        return loss_value\n",
        "        \n",
        "    if i % 3 == 0:\n",
        "        loss_value = step(\"w0_loc\")\n",
        "    if i % 3 == 1:\n",
        "        loss_value = step(\"w1_loc\")\n",
        "    if i % 3 == 2:\n",
        "        loss_value = step(\"w2_loc\")\n",
        "        \n",
        "    if (i+1) % 900 == 0 or (i==0):\n",
        "        print(loss_value.detach().numpy())"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "44.325314\n",
            "37.59131\n",
            "30.394562\n",
            "55.613743\n",
            "61.8145\n",
            "182.16685\n",
            "25.236744\n",
            "229.55133\n",
            "33.00921\n",
            "26.89841\n",
            "12.4200115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zpNLjyWsgwOG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "4cca6ebe-c8a5-4463-98fc-fd5edede7489"
      },
      "cell_type": "code",
      "source": [
        "for k, v in variational_params.items():\n",
        "    print(\"{} : {}\".format(k, v))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w0_loc : 8.712218004802708e-06\n",
            "w0_scale_log : 0.0\n",
            "w1_loc : -1.857791721704416e-05\n",
            "w1_scale_log : 0.0\n",
            "w2_loc : -1.4663711226603482e-05\n",
            "w2_scale_log : 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hu02ZTg-jZv0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}