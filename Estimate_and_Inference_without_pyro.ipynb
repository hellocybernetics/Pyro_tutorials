{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hellocybernetics/Pyro_tutorials/blob/master/Estimate_and_Inference_without_pyro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hA-qlJZ9Nw4v",
        "colab_type": "code",
        "outputId": "6933a9e7-3647-4713-8da4-a2ff57a397fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pyro-ppl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyro-ppl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/5e/456d5bc24c010996c6d39ebe47680a5fa72c09b08146c15df23ba8945680/pyro-ppl-0.3.0.tar.gz (204kB)\n",
            "\u001b[K    100% |████████████████████████████████| 204kB 25.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.5.5)\n",
            "Requirement already satisfied: graphviz>=0.8 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.10.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (2.2)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.14.6)\n",
            "Collecting opt_einsum>=2.3.0 (from pyro-ppl)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/d6/44792ec668bcda7d91913c75237314e688f70415ab2acd7172c845f0b24f/opt_einsum-2.3.2.tar.gz (59kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 22.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.11.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.0.0)\n",
            "Requirement already satisfied: tqdm>=4.28 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (4.28.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.2->pyro-ppl) (4.3.2)\n",
            "Building wheels for collected packages: pyro-ppl, opt-einsum\n",
            "  Building wheel for pyro-ppl (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ed/b5/01/c883fa6e02eb51fd67ba2b73f1ddcee42997716394792d57da\n",
            "  Building wheel for opt-einsum (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/51/3e/a3/b351fae0cbf15373c2136a54a70f43fea5fe91d8168a5faaa4\n",
            "Successfully built pyro-ppl opt-einsum\n",
            "Installing collected packages: opt-einsum, pyro-ppl\n",
            "Successfully installed opt-einsum-2.3.2 pyro-ppl-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "459cvjGfOGyN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pyroの基本的な役割\n",
        "PyroはPyTorchを計算のバックエンドに構えた確率プログラミング言語です。確率プログラミング言語の主な役割は、様々な確率分布からのサンプリングや、同時分布・条件付き分布・周辺分布の取扱を容易にすることです。**まずはTorchを使ってみて、その確率プログラミング言語の必要性を体感してみましょう。**"
      ]
    },
    {
      "metadata": {
        "id": "77PKI6F3N6Ec",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.distributions as torchdist\n",
        "\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "import pyro.poutine as poutine"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LSELqDR0PTtu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 分布の記述\n",
        "まず、肩慣らしに標準正規分布 $\\rm {Normal} (0, 1)$ を書いてみましょう。`torch.distributions`  モジュールを使うことで下記のように記述することができます。"
      ]
    },
    {
      "metadata": {
        "id": "bPIXw1EnPDU2",
        "colab_type": "code",
        "outputId": "1b1c234b-af29-4d91-e7e4-221b9fce1b0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "normal_dist = torchdist.Normal(loc=torch.tensor(0.), scale=torch.tensor(1.))\n",
        "normal_dist"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Normal(loc: 0.0, scale: 1.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "k93zrklnPuis",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### サンプリング\n",
        "続いて、記述した分布から\n",
        "$$\n",
        "x \\sim {\\rm Normal}(0, 1)\n",
        "$$\n",
        "とサンプリングを得るには下記のようにします。"
      ]
    },
    {
      "metadata": {
        "id": "9Apgnf6sQFe0",
        "colab_type": "code",
        "outputId": "20b0ec5b-91ee-4f95-de95-257c78002529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "x = normal_dist.sample()\n",
        "x"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0399)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "mq-6TcUIQPqS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "更に同じ分布から独立に複数のサンプルを得たい場合には下記のように、`sample()` メソッドに引数を渡すことで実施します。このとき引数はリストあるいはタプルで渡すようにします。"
      ]
    },
    {
      "metadata": {
        "id": "kc0-utrYQnVK",
        "colab_type": "code",
        "outputId": "0a98a116-f29c-4ccf-e921-d7bdc58ab211",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "X1 = normal_dist.sample([3,])\n",
        "X1"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.0747, -0.5219, -0.5010])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "Puq8R9cqRihc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "上記のサンプルは各成分が各々独立に正規分布から生成されています。数式で表現するならば下記のようになります。\n",
        "\n",
        "$$\n",
        "X_1[i] \\sim \\rm{Normal}(0, 1)\n",
        "$$\n",
        "\n",
        "同じ理屈でもっと多次元の配列を作ることもできます。"
      ]
    },
    {
      "metadata": {
        "id": "VzE_FNBCQqKa",
        "colab_type": "code",
        "outputId": "192bd231-a88e-477e-869e-61e0784e8450",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "cell_type": "code",
      "source": [
        "X2 = normal_dist.sample([10, 2])\n",
        "X2"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.8258, -0.3836],\n",
              "        [ 1.3726, -0.6616],\n",
              "        [ 1.5686, -0.8907],\n",
              "        [-0.4262, -1.2156],\n",
              "        [ 2.3546, -0.6246],\n",
              "        [ 0.6792, -0.8905],\n",
              "        [ 0.2586,  0.1164],\n",
              "        [ 1.6033,  1.4882],\n",
              "        [ 0.7288,  0.5897],\n",
              "        [-0.2297, -0.1686]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "yfcsEdNdRvic",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "上記の配列も同様に各成分が独立に正規分布から生成されています。\n",
        "\n",
        "$$\n",
        "X_2[i, j] = \\rm{Normal}(0, 1)\n",
        "$$\n",
        "\n",
        "配列として取り出すと、もはや元々これが何の分布の話だったのか、情報を持ち合わせていないので注意が必要です。例えば、下記のような2次元正規分布を考えましょう。2次元正規分布からの1つのサンプルは当然2つの要素を持っています。"
      ]
    },
    {
      "metadata": {
        "id": "K5GrSGa7RBAK",
        "colab_type": "code",
        "outputId": "abbe135a-4ae3-4c95-dfa5-5ec636e587d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "multi_normal_dist = torchdist.MultivariateNormal(\n",
        "    loc=torch.tensor([1., -2.]),\n",
        "    covariance_matrix=torch.tensor([[1., -1.],\n",
        "                                    [-1., 2.]])\n",
        ")\n",
        "\n",
        "multi_normal_dist.sample()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 2.6118, -4.3481])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "8gKtZFYpVB0C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "この2次元正規分布から10個のサンプルを得ると、`(10, 2)` の `tensor`が得られますが、この `tensor` の各成分は独立ではありません。各行は独立ですが、各列は2次元の正規分布からの一つのサンプルであり、相関を持っていることに注意する必要があります。\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "M1BGa6oXUILa",
        "colab_type": "code",
        "outputId": "b7876e97-8fc2-42a2-b137-3e3a34d73473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "cell_type": "code",
      "source": [
        "multi_normal_dist.sample([10])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0605, -2.3511],\n",
              "        [ 0.8840, -0.7532],\n",
              "        [-0.7491,  0.0411],\n",
              "        [ 0.4643, -1.0314],\n",
              "        [-1.5282, -1.8038],\n",
              "        [ 0.1993, -0.6630],\n",
              "        [ 0.9128, -2.5010],\n",
              "        [ 1.8150, -2.7232],\n",
              "        [ 0.7102, -2.0912],\n",
              "        [-0.4182, -2.7429]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "sHtUrjA7VszA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "長いコードを書いているうちに、ある`tensor` がどういうサンプルのされ方をしたものであったのかを見失うことも起こりうるので注意が必要です。"
      ]
    },
    {
      "metadata": {
        "id": "ED47SK9QVuBv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 尤度の計算\n",
        "機械学習ではしばしば確率分布 $p(x)$ の尤度の計算が必要になります。あるサンプル $x^*$ の $p(x)$ における尤度は単純に $p(x^*)$ の値になります。では $\\{x_1, x_2, x_3,...,\\}$ の尤度はどのように計算するのかというと、\n",
        "\n",
        "$$\n",
        "p(x_1)p(x_2)p(x_3)...\n",
        "$$\n",
        "\n",
        "と各確率値の積によって表されます。確率値は $[0, 1]$ の値でありサンプルの数が多くなると、$1$ 未満の数の積が連なり非常に小さな値となってしまいます。そこで、コンピュータでは通常、対数尤度というものを計算することにします。\n",
        "\n",
        "$$\n",
        "{\\rm log}\\{p(x_1)p(x_2)p(x_3)...\\} = {\\rm log}p(x_1) + {\\rm log}p(x_2) + {\\rm log}p(x_3) + ...\n",
        "$$\n",
        "\n",
        "と負の値の足し算になり数値的に安定します。また、$\\rm log$ は単調増加関数であるので、微分にとっても符号には影響が無く（絶対値には影響するが）、積が和となっているため都合が良いです。サンプル $x^*$ の対数尤度は簡単に下記のコードで求まります。"
      ]
    },
    {
      "metadata": {
        "id": "1LnrmXtHWQs8",
        "colab_type": "code",
        "outputId": "e100de5a-fde7-4bd1-c3e4-efe8f94424f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "sample = multi_normal_dist.sample()\n",
        "print(\"*** a sample from 2dim normal: \\n\", sample)\n",
        "log_likelihood = multi_normal_dist.log_prob(sample)\n",
        "print(\"*** log likelihood of the sample \\n\", log_likelihood)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** a sample from 2dim normal: \n",
            " tensor([ 0.1378, -1.1663])\n",
            "*** log likelihood of the sample \n",
            " tensor(-2.2100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qZb5n79oWebI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "また複数のサンプルに関しては足し算する前の各々の対数尤度が返ってきます。"
      ]
    },
    {
      "metadata": {
        "id": "KRHth-wsYfF-",
        "colab_type": "code",
        "outputId": "1ef1fc1c-d65a-4a95-8fb2-0931fdfd1872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "samples = multi_normal_dist.sample([5,])\n",
        "print(\"*** 5 samples from 2dim normal: \\n\", sample)\n",
        "log_likelihoods = multi_normal_dist.log_prob(samples)\n",
        "print(\"*** log likelihoods of the samples: \\n\", log_likelihoods)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** 5 samples from 2dim normal: \n",
            " tensor([ 0.1378, -1.1663])\n",
            "*** log likelihoods of the samples: \n",
            " tensor([-2.1829, -2.0011, -2.6910, -2.2002, -1.8725])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wy4ONNQrYkgi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### MAP推定\n",
        "ここでMAP推定を実施してみます。上記の関数と自動微分機能を駆使すれば難しくありません。\n",
        "\n",
        "#### 訓練データ\n",
        "人工訓練データを作りますが、パラメータは知らない体で推定をします。"
      ]
    },
    {
      "metadata": {
        "id": "M1mu_XCBZ-8e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def toy_data():\n",
        "    true_mu = torch.tensor([2., 3.])\n",
        "    true_cov = torch.tensor([[2., -3.],[-3, 5]])\n",
        "    \n",
        "    X = torchdist.MultivariateNormal(loc=true_mu, \n",
        "                                     covariance_matrix=true_cov).sample([100,])\n",
        "    return X\n",
        "\n",
        "X_train = toy_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RePK_G1Kac-j",
        "colab_type": "code",
        "outputId": "6d01ace1-f9a2-48cc-a5d3-dd201b5f4d2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(X_train.numpy()[:,0], X_train.numpy()[:,1], \"o\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0c06693780>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAFKCAYAAABcq1WoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvgAAuIzwBQDAZf8Puob9DtkLZMgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "2C3CoLtMbFP-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 推定のためのモデル\n",
        "データを可視化して、コレは2次元正規分布に従っていると睨んだとしましょう。すると私達が推定したいパラメータは、平均ベクトルと共分散行列になります。これらをまず初期化しておきます。"
      ]
    },
    {
      "metadata": {
        "id": "O5WouLd4beCO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mu_param = torch.nn.Parameter(torch.tensor([0., 0.]))\n",
        "cov_tril_param = torch.nn.Parameter(torch.tensor([[1., 0.],\n",
        "                                                  [0., 1.]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IdOxDc-Nc0gP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 対数尤度関数（最適化の目的関数）\n",
        "データ $X = \\{x_1, \\cdots, x_N\\}$ に対する対数尤度関数は\n",
        "\n",
        "$$\n",
        "{\\rm LogLikelihood}(X, \\mu, \\Sigma) = \\frac{1}{N}\\sum_{i=1}^N {\\rm log}\\{{\\rm Normal}(x_i \\mid \\mu, \\Sigma)\\}\n",
        "$$\n",
        "\n",
        "であり、データ $X$ は既に手元にあるので、パラメータ $\\mu, \\Sigma$ に関して最大化するのが最尤推定法になります。続いて事前分布 $p(\\mu)$ と $p(\\Sigma)$ を目的関数に考慮するのがMAP推定になります。具体的には下記のように修正されます。\n",
        "\n",
        "$$\n",
        "{\\rm LogJointProb}(X, \\mu, \\Sigma) = \\frac{1}{N}\\sum_{i=1}^N {\\rm log}\\{{\\rm Normal}(x_i \\mid \\mu, \\Sigma)\\} + {\\rm log}p(\\mu) +{\\rm log}p(\\Sigma)\n",
        "$$\n",
        "\n",
        "今回は適当に分散の大きな正規分布を仮定し、無情報事前分布に近い状態にしておきます。\n",
        "\n",
        "#### 補足\n",
        "ちなみにベイズの定理よりパラメータの事後分布は\n",
        "\n",
        "$$\n",
        "p(\\mu, \\Sigma \\mid X) = \\frac{p(X, \\mu, \\Sigma)}{p(X)}\n",
        "$$\n",
        "\n",
        "と表され、この分母は定数になっています。したがって分子の同時分布について最大化すれば良いというのかMAP推定であり、同時分布を\n",
        "\n",
        "$$\n",
        "p(X,\\mu,\\Sigma)=p(X\\mid \\mu, \\Sigma) p(\\mu) p(\\Sigma)\n",
        "$$\n",
        "\n",
        "と表現しつつ、対数を取ることで上記の目的関数が導出されます。"
      ]
    },
    {
      "metadata": {
        "id": "Iya1lNR9b2Eu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def log_joint_prob(mu_param, cov_param, X):\n",
        "    \n",
        "    prior_mu = torchdist.Normal(loc=torch.zeros_like(mu_param),\n",
        "                                 scale=100*torch.ones_like(mu_param))\n",
        "    prior_cov = torchdist.Normal(loc=torch.zeros_like(cov_param),\n",
        "                                 scale=100*torch.ones_like(cov_param))\n",
        "    \n",
        "    model = torchdist.MultivariateNormal(\n",
        "        loc=mu_param, \n",
        "        scale_tril=cov_param\n",
        "    )\n",
        "    \n",
        "    return (\n",
        "        model.log_prob(X).mean() \n",
        "        + prior_mu.log_prob(mu_param).mean()\n",
        "        + prior_cov.log_prob(cov_param).mean()\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mkk01MKRd2UZ",
        "colab_type": "code",
        "outputId": "fc740c2d-1ba9-448b-df37-31f65e515a75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"*** initial log joint prob\")\n",
        "log_joint_prob(mu_param, cov_tril_param, X_train)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** initial log joint prob\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-23.5550, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "_bOHbf89kWuo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=[mu_param, cov_tril_param], lr=1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FyyuYqyEkw3b",
        "colab_type": "code",
        "outputId": "9d93a230-69f1-4f9f-ddad-9a9855466d8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(10000):\n",
        "    optimizer.zero_grad()\n",
        "    log_joint_prob_value = log_joint_prob(mu_param, cov_tril_param, X_train)\n",
        "    loss_value = - log_joint_prob_value\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (i+1) % 1000 == 0 or (i==0):\n",
        "        print(loss_value.detach().numpy())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23.554953\n",
            "16.814312\n",
            "15.548412\n",
            "14.782041\n",
            "14.4432125\n",
            "14.053167\n",
            "13.900515\n",
            "13.896851\n",
            "13.896435\n",
            "13.896425\n",
            "13.896425\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w0AAZ-oPlakF",
        "colab_type": "code",
        "outputId": "eecefaa4-8db4-46f5-af10-783271b18566",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def toy_data():\n",
        "    true_mu = torch.tensor([2., 3.])\n",
        "    true_cov = torch.tensor([[2., -3.],\n",
        "                             [-3., 5.]])\n",
        "    \n",
        "    X = torchdist.MultivariateNormal(loc=true_mu, \n",
        "                                     covariance_matrix=true_cov).sample([100,])\n",
        "    return X\n",
        "\"\"\"\n",
        "\n",
        "print(\"mu map estimated: \\n\", mu_param.data)\n",
        "print(\"cov map estimated \\n\", cov_tril_param.mm(cov_tril_param.t()).data)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mu map estimated: \n",
            " tensor([2.0524, 2.9367])\n",
            "cov map estimated \n",
            " tensor([[ 2.3883, -3.6838],\n",
            "        [-3.6838,  6.1090]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5qLM4A32m_Ko",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 回帰モデル\n",
        "次に同じようにTorchで回帰モデルを実装していきます。スカラーの入力 $x$ とスカラーの出力 $y$ の関係性が下記の用に多項式で表されると仮定しましょう。\n",
        "\n",
        "$$\n",
        "y = -3 + 4x + x^2 + \\epsilon\n",
        "$$\n",
        "\n",
        "ただし、ここで $\\epsilon \\sim {\\rm Normal}(0, 1)$ の正規乱数としておきます。"
      ]
    },
    {
      "metadata": {
        "id": "oRiDD1Ux1vP4",
        "colab_type": "code",
        "outputId": "3f87358b-cda6-4364-8898-914fb9716a7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "cell_type": "code",
      "source": [
        "def toy_poly():\n",
        "    \n",
        "    x = 5 * torch.rand(100, 1) \n",
        "    linear_op = -3 - 4*x + 1*x**2 \n",
        "    y = torchdist.Normal(linear_op, 1).sample()\n",
        "    return x, y\n",
        "\n",
        "x_train, y_train = toy_poly()\n",
        "\n",
        "plt.plot(x_train.numpy(), y_train.numpy(), \"o\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0c0462ecf8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAFKCAYAAABcq1WoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "G4wncEBU2n3x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### モデル\n",
        "さて、上記のデータを見て、二次関数で表現できると睨んだとしましょう（出来レースですが）。\n",
        "\n",
        "$$\n",
        "y = w_0 + w_1x + w_2x^2 + \\epsilon\n",
        "$$\n",
        "\n",
        "すると、簡単な式変形から下記のような確率モデルを使うことが考えられます（$\\epsilon$の分散は既知としているが、未知にすることも当然必要であれば考えられる）。 \n",
        "\n",
        "\n",
        "$$\n",
        "y - (w_0 + w_1x + w_2x^2) \\sim {\\rm Normal}(0,1)\n",
        "$$\n",
        "\n",
        "あるいは、分布の平均の方にパラメータを持つ項を吸収させて\n",
        "\n",
        "$$\n",
        "y \\sim {\\rm Normal}(w_0 + w_1x + w_2x^2 ,1)\n",
        "$$\n",
        "\n",
        "とできます。事前分布を適当に置いてしまえば、MAP推定に必要な同時分布 $p(w_0, w_1, w_2, x, y)$ が一先ず書き下せますので、その対数値を最大化することで回帰問題を解くことが可能になります。今回も各パラメータに対してはそれぞれ適当な正規分布を仮定してしまいましょう。\n",
        "\n",
        "$$\n",
        "{\\rm LogJointProb}(w_0, w_1, w_2, X, Y) = \\frac{1}{N} \\sum_{i=1}^N {\\rm Normal}(y_i\\mid w_0 + w_1x_i + w_2x_i^2 ,1) +{\\rm log} p(w_0) +{\\rm log} p(w_1) +{\\rm log} p(w_2)\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "id": "LmDgr1854-ZS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w0 = torch.nn.Parameter(torch.tensor(1.))\n",
        "w1 = torch.nn.Parameter(torch.tensor(1.))\n",
        "w2 = torch.nn.Parameter(torch.tensor(1.))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yM3nPHcf5J25",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def log_joint_prob(w0, w1, w2, x, y):\n",
        "    \n",
        "    prior_w0 = torchdist.Normal(torch.tensor(0.), 10*torch.tensor(1.))\n",
        "    prior_w1 = torchdist.Normal(torch.tensor(0.), 10*torch.tensor(1.))\n",
        "    prior_w2 = torchdist.Normal(torch.tensor(0.), 10*torch.tensor(1.))\n",
        "\n",
        "    linear = w0 + w1*x + w2*x**2\n",
        "    likelihood = torchdist.Normal(linear, torch.ones_like(linear))\n",
        "    \n",
        "    return (\n",
        "        prior_w0.log_prob(w0).mean() +\n",
        "        prior_w1.log_prob(w1).mean() +\n",
        "        prior_w2.log_prob(w2).mean() +\n",
        "        likelihood.log_prob(y).mean()\n",
        "    )    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DcnPsxO-T9Ie",
        "colab_type": "code",
        "outputId": "0915ec28-9efb-410f-db07-e41f37c2397b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=[w0, w1, w2], lr=1e-3)\n",
        "\n",
        "for i in range(30000):\n",
        "    optimizer.zero_grad()\n",
        "    log_joint_prob_value = log_joint_prob(w0, w1, w2, x_train, y_train)\n",
        "    loss_value = - log_joint_prob_value\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (i+1) % 1000 == 0 or (i==0):\n",
        "        print(loss_value.detach().numpy())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "159.7145\n",
            "37.539642\n",
            "20.450272\n",
            "18.480148\n",
            "16.616161\n",
            "14.740344\n",
            "13.157278\n",
            "12.070682\n",
            "11.507385\n",
            "11.308815\n",
            "11.248821\n",
            "11.209596\n",
            "11.179522\n",
            "11.163915\n",
            "11.159591\n",
            "11.159147\n",
            "11.159139\n",
            "11.15914\n",
            "11.159138\n",
            "11.159138\n",
            "11.15914\n",
            "11.159139\n",
            "11.159141\n",
            "11.159139\n",
            "11.159139\n",
            "11.159139\n",
            "11.159139\n",
            "11.159139\n",
            "11.159139\n",
            "11.159138\n",
            "11.159139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vfoligfvUrqV",
        "colab_type": "code",
        "outputId": "00684adf-361c-4093-99c2-b938bf72bd43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def toy_poly():\n",
        "    \n",
        "    x = 5 * torch.rand(100, 1) \n",
        "    linear_op = -3 - 4*x + 1*x**2 \n",
        "    y = torchdist.Normal(linear_op, 1).sample()\n",
        "    return x, y\n",
        "\n",
        "\"\"\"\n",
        "print(w0.data)\n",
        "print(w1.data)\n",
        "print(w2.data)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-3.1003)\n",
            "tensor(-3.9002)\n",
            "tensor(0.9775)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hh15BgdT-v3M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 変分推論\n",
        "ベイズ推論は事後分布 \n",
        "\n",
        "$$\n",
        "p(\\theta \\mid D) = \\frac{p(D\\mid \\theta)p(\\theta)}{p(D)}\n",
        "$$\n",
        "\n",
        "において、同時分布の最大化に甘んじず（すなわち関数の一番山となる部分だけを探すのではなく）、分布の形状全体を把握しようという試みになります。その試みは一般に困難を極めます。都合の良い尤度関数と都合の良い事前分布を選ばない限りは形状全体を上手に求めることはできません。\n",
        "\n",
        "したがって、形状全体を知りたいのだが、ある程度簡略化した形状で一番近いものを探せればいいというのが変分推論です。変分モデル $q(\\theta; \\eta)$ を仮定し（$\\eta$は変分パラメータと呼ばれる最適化すべきパラメータである）、$\\eta$ の調整で分布の形状を変えること $p(\\theta |D)$ に最も近い $q(\\theta; \\eta)$ を決定します。近いというのはKLダイバージェンスの意味であり\n",
        "\n",
        "$$\n",
        "{\\rm KL}[q(\\theta; \\eta) : p(\\theta\\mid D)] = {\\mathbb E}_{q(\\theta;\\eta)}[{\\rm log}q(\\theta; \\eta)] - {\\mathbb E}_{q(\\theta;\\eta)}[{\\rm log}p(\\theta)] - {\\mathbb E}_{q(\\theta;\\eta)}[{\\rm log}p(D\\mid\\theta)]\n",
        "$$\n",
        "\n",
        "を最小化するような $\\eta$ を求めます。\n",
        "\n",
        "こちらも最適化問題ではありますが、求めているものはパラメータ $\\theta $ の値ではなくパラメータ $\\theta$ が取りうる値の分布を網羅的に把握するために $\\eta$ を最適化していることに注意しましょう。最適化された $\\eta$ によって分布 $q(\\theta ; \\eta)$ が定まり、この分布からサンプリングをしたりすることで、単に点推定で $\\theta$ を決めてしまうよりも多くの情報を利用することができるというわけです。\n",
        "\n",
        "実際の推論では期待値計算の代わりに現在の $\\eta$ の値を用いて \n",
        "\n",
        "$$\n",
        "\\theta^* \\sim q(\\theta; \\eta)\n",
        "$$\n",
        "\n",
        "とサンプリングし、サンプリングされた$\\theta^*$ で現在のKLダイバージェンスを計算するということにします（そんなのいい加減すぎる！と思うのであれば、$q(\\theta; \\eta)$ は現在の $\\eta$ を使うとして重点サンプリングなどをしてもいいだろうし、大げさにもMCMCを使ってもいいだろう。単に計算量の問題である）。\n",
        "\n",
        "$$\n",
        "{\\rm KL}[q(\\theta; \\eta) : p(\\theta\\mid D)] \\simeq {\\rm log}q(\\theta^*; \\eta) - {\\rm log}p(\\theta^*) - {\\rm log}p(D\\mid\\theta^*)\n",
        "$$\n",
        "\n",
        "\n",
        "#### 変分モデル\n",
        "回帰問題の例に戻って、パラメータ $w_0, w_1, w_2$ に対してそれぞれ変分モデル\n",
        "\n",
        "$$\n",
        "q(w_i ; \\eta_i) = {\\rm Normal}(\\mu_i, \\sigma_i)\n",
        "$$\n",
        "\n",
        "を仮定しましょう。すなわち各 $w_i$ に対して正規分布を仮定して、あとはそれぞれの平均分散を変分パラメータとして最適化して $w_i$ の分布を得てしまおうということにしたのです。"
      ]
    },
    {
      "metadata": {
        "id": "n4xTF9EUCD-D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "variational_params = {\n",
        "    \"w0_loc\": torch.nn.Parameter(torch.tensor(0.)),\n",
        "    \"w0_scale_log\": torch.nn.Parameter(torch.tensor(0.)),\n",
        "    \"w1_loc\": torch.nn.Parameter(torch.tensor(0.)),\n",
        "    \"w1_scale_log\": torch.nn.Parameter(torch.tensor(0.)),\n",
        "    \"w2_loc\": torch.nn.Parameter(torch.tensor(0.)),\n",
        "    \"w2_scale_log\": torch.nn.Parameter(torch.tensor(0.)),\n",
        "}\n",
        "\n",
        "def variational_model(variational_params):\n",
        "    \"\"\"\n",
        "    Variational model q(w; eta)\n",
        "    arg: variational parameters \"eta\"\n",
        "    return: w ~ q(w; eta)\n",
        "    \"\"\"\n",
        "    w0_q = torchdist.Normal(\n",
        "        variational_params[\"w0_loc\"],\n",
        "        torch.exp(variational_params[\"w0_scale_log\"]),\n",
        "    )\n",
        "    \n",
        "    w1_q = torchdist.Normal(\n",
        "        variational_params[\"w1_loc\"],\n",
        "        torch.exp(variational_params[\"w1_scale_log\"]),\n",
        "    )\n",
        "    \n",
        "    w2_q = torchdist.Normal(\n",
        "        variational_params[\"w2_loc\"],\n",
        "        torch.exp(variational_params[\"w2_scale_log\"]),\n",
        "    )\n",
        "    \n",
        "    return w0_q, w1_q, w2_q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "huYHejDTJOSy",
        "colab_type": "code",
        "outputId": "aef31057-0cb1-4ad3-8908-aece90cb4afc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"*** w0, w1, w2 :variational model with initial variatinal params\")\n",
        "variational_model(variational_params)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** w0, w1, w2 :variational model with initial variatinal params\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Normal(loc: 0.0, scale: 1.0),\n",
              " Normal(loc: 0.0, scale: 1.0),\n",
              " Normal(loc: 0.0, scale: 1.0))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "UrU74P8QJdKv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### KLダイバージェンス\n",
        "変分モデルを書き終えたので、後は変分モデルから出てくるサンプルを引数としたKLダイバージェンスを書き下せばいいです。KLダイバージェンスの近似は実は下記の通り、MAP推定でも利用してきたLog Joint Prob（データ $D$とパラメータ $\\theta$の対数同時確率）\n",
        "と新たに出てきたVariational model （勝手に仮定したパラメータ $\\theta $ の事後分布）の対数確率値で構成されています。\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "{\\rm KL}[q(\\theta; \\eta) : p(\\theta\\mid D)] &\\simeq {\\rm log}q(\\theta^*; \\eta) - {\\rm log}p(\\theta^*) - {\\rm log}p(D\\mid\\theta^*) \\\\\\ \n",
        "& = {\\rm log}q(\\theta^*; \\eta) - {\\rm LogJointProb}(\\theta^*, D)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "ですので、実装上はMAP推定に使っていた目的関数をそのまま流用できます。\n",
        "\n",
        "すなわち実装は、$\\eta$ の関数である変分モデルを作り、パラメータ $\\theta$ をサンプリングできるようにして、そのサンプリング値$\\theta^*$をMAP推定で使っていた目的関数に代入してやれば良いのです。更に、サンプリングされた $\\theta^*$の$q(\\theta; \\eta)$ におけるの対数確率値も追加してやれば、目的関数の作成は終了です。"
      ]
    },
    {
      "metadata": {
        "id": "fkMzu9zJNNPp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def kl_divergence(variational_params, x, y):\n",
        "    w0_q, w1_q, w2_q = variational_model(variational_params)\n",
        "    \n",
        "    w0_sample = w0_q.sample()\n",
        "    w1_sample = w1_q.sample()    \n",
        "    w2_sample = w2_q.sample()\n",
        "    \n",
        "    log_joint_prob_value = log_joint_prob(w0_sample, w1_sample, w2_sample, x, y)\n",
        "    log_variational_prob_value = (\n",
        "        w0_q.log_prob(w0_sample) +\n",
        "        w1_q.log_prob(w1_sample) +\n",
        "        w2_q.log_prob(w2_sample)\n",
        "    )\n",
        "    \n",
        "    return log_variational_prob_value - log_joint_prob_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-eCSuQ4cP2ua",
        "colab_type": "code",
        "outputId": "25dd7250-4705-4b1d-e0e5-9c4f77afdc2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(params=variational_params.values(), lr=1e-8)\n",
        "\n",
        "for i in range(9000):\n",
        "    optimizer.zero_grad()\n",
        "    loss_value =kl_divergence(variational_params, x_train, y_train)\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (i+1) % 300 == 0 or (i==0):\n",
        "        print(loss_value.detach().numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51.307236\n",
            "30.228342\n",
            "24.399588\n",
            "94.61427\n",
            "54.46742\n",
            "256.63162\n",
            "46.625977\n",
            "121.564\n",
            "83.602974\n",
            "36.613064\n",
            "38.916714\n",
            "52.55144\n",
            "46.949112\n",
            "29.521364\n",
            "66.69088\n",
            "175.45775\n",
            "31.18562\n",
            "46.020782\n",
            "128.05133\n",
            "28.738707\n",
            "21.411339\n",
            "122.938194\n",
            "56.800346\n",
            "67.371475\n",
            "24.99657\n",
            "57.510315\n",
            "18.435207\n",
            "26.601477\n",
            "33.188393\n",
            "34.200565\n",
            "61.512882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-z6gnrVnWD8C",
        "colab_type": "code",
        "outputId": "664b6f37-7983-44e4-9987-31d176704193",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "cell_type": "code",
      "source": [
        "for k, v in variational_params.items():\n",
        "    print(\"{} : {}\".format(k, v))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w0_loc : 1.2164202871645102e-06\n",
            "w0_scale_log : -6.172373900881212e-07\n",
            "w1_loc : -1.4097398661760963e-06\n",
            "w1_scale_log : 9.952343589247903e-07\n",
            "w2_loc : 1.3103788205626188e-06\n",
            "w2_scale_log : -2.068474259431241e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VBfVH_TZYnIS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 学習の仕方を変える\n",
        "さて、結果は上手く言ったと言えるのでしょうか…？どうやらイマイチな気がしますね。変分推論では、変分パラメータを同時に一気に最適化するのは難しいので、実際には変分パラメータを1つずつ更新していく必要があります。\n",
        "\n",
        "具体的には学習のコードを下記のように書き直します。まずは `optimizer` を各々のパラメータに対して設定し、個別にパラメータを1つずつ更新できるようにしておきましょう。"
      ]
    },
    {
      "metadata": {
        "id": "R77Q_0INddb2",
        "colab_type": "code",
        "outputId": "7f7152a0-1dca-41c9-99cd-dd2a39551035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        }
      },
      "cell_type": "code",
      "source": [
        "optimizers = {}\n",
        "for key in variational_params.keys():\n",
        "    optimizers[key] = torch.optim.SGD(params=[variational_params[key]], lr=1e-6)\n",
        "\n",
        "optimizers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'w0_loc': SGD (\n",
              " Parameter Group 0\n",
              "     dampening: 0\n",
              "     lr: 1e-06\n",
              "     momentum: 0\n",
              "     nesterov: False\n",
              "     weight_decay: 0\n",
              " ), 'w0_scale_log': SGD (\n",
              " Parameter Group 0\n",
              "     dampening: 0\n",
              "     lr: 1e-06\n",
              "     momentum: 0\n",
              "     nesterov: False\n",
              "     weight_decay: 0\n",
              " ), 'w1_loc': SGD (\n",
              " Parameter Group 0\n",
              "     dampening: 0\n",
              "     lr: 1e-06\n",
              "     momentum: 0\n",
              "     nesterov: False\n",
              "     weight_decay: 0\n",
              " ), 'w1_scale_log': SGD (\n",
              " Parameter Group 0\n",
              "     dampening: 0\n",
              "     lr: 1e-06\n",
              "     momentum: 0\n",
              "     nesterov: False\n",
              "     weight_decay: 0\n",
              " ), 'w2_loc': SGD (\n",
              " Parameter Group 0\n",
              "     dampening: 0\n",
              "     lr: 1e-06\n",
              "     momentum: 0\n",
              "     nesterov: False\n",
              "     weight_decay: 0\n",
              " ), 'w2_scale_log': SGD (\n",
              " Parameter Group 0\n",
              "     dampening: 0\n",
              "     lr: 1e-06\n",
              "     momentum: 0\n",
              "     nesterov: False\n",
              "     weight_decay: 0\n",
              " )}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "metadata": {
        "id": "v7kEd7N9dweL",
        "colab_type": "code",
        "outputId": "358062f6-5fe9-46ce-920f-08ae3d0956d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(9000):\n",
        "\n",
        "    def step(key):\n",
        "        optimizers[key].zero_grad()\n",
        "        loss_value =kl_divergence(variational_params, x_train, y_train)\n",
        "        loss_value.backward()\n",
        "        optimizers[key].step()\n",
        "        return loss_value\n",
        "        \n",
        "    if i % 3 == 0:\n",
        "        loss_value = step(\"w0_loc\")\n",
        "    if i % 3 == 1:\n",
        "        loss_value = step(\"w1_loc\")\n",
        "    if i % 3 == 2:\n",
        "        loss_value = step(\"w2_loc\")\n",
        "        \n",
        "    if (i+1) % 900 == 0 or (i==0):\n",
        "        print(loss_value.detach().numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "44.325314\n",
            "37.59131\n",
            "30.394562\n",
            "55.613743\n",
            "61.8145\n",
            "182.16685\n",
            "25.236744\n",
            "229.55133\n",
            "33.00921\n",
            "26.89841\n",
            "12.4200115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zpNLjyWsgwOG",
        "colab_type": "code",
        "outputId": "4cca6ebe-c8a5-4463-98fc-fd5edede7489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "cell_type": "code",
      "source": [
        "for k, v in variational_params.items():\n",
        "    print(\"{} : {}\".format(k, v))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w0_loc : 8.712218004802708e-06\n",
            "w0_scale_log : 0.0\n",
            "w1_loc : -1.857791721704416e-05\n",
            "w1_scale_log : 0.0\n",
            "w2_loc : -1.4663711226603482e-05\n",
            "w2_scale_log : 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hu02ZTg-jZv0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 入力変数の無相関化\n"
      ]
    },
    {
      "metadata": {
        "id": "uiqchvsFKoxt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w0 = torch.nn.Parameter(torch.tensor(1.))\n",
        "w1 = torch.nn.Parameter(torch.tensor(1.))\n",
        "w2 = torch.nn.Parameter(torch.tensor(1.))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RPK6G7ZlLeJU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def log_joint_prob(w1, w2, x, y):\n",
        "    \n",
        "    z1 = x\n",
        "    z2 = x**2\n",
        "    \n",
        "    z1 = z1 - z1.mean()\n",
        "    z2 = z2 - z2.mean()\n",
        "    \n",
        "#      w0 = learned_w1 * x.mean() + learned_w2 * x2.mean()\n",
        "#     prior_w0 = torchdist.Normal(torch.tensor(0.), 10*torch.tensor(1.))\n",
        "    prior_w1 = torchdist.Normal(torch.tensor(0.), 10*torch.tensor(1.))\n",
        "    prior_w2 = torchdist.Normal(torch.tensor(0.), 10*torch.tensor(1.))\n",
        "\n",
        "    linear = w1*z1 + w2*z2\n",
        "    likelihood = torchdist.Normal(linear, torch.ones_like(linear))\n",
        "    \n",
        "    return (\n",
        "#         prior_w0.log_prob(w0).mean() +\n",
        "        prior_w1.log_prob(w1).mean() +\n",
        "        prior_w2.log_prob(w2).mean() +\n",
        "        likelihood.log_prob(y).mean()\n",
        "    )    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZghLNLBcMCAS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "variational_params = {\n",
        "#     \"w0_loc\": torch.nn.Parameter(torch.tensor(0.)),\n",
        "#     \"w0_scale_log\": torch.nn.Parameter(torch.tensor(0.)),\n",
        "    \"w1_loc\": torch.nn.Parameter(torch.tensor(0.)),\n",
        "    \"w1_scale_log\": torch.nn.Parameter(torch.tensor(0.)),\n",
        "    \"w2_loc\": torch.nn.Parameter(torch.tensor(0.)),\n",
        "    \"w2_scale_log\": torch.nn.Parameter(torch.tensor(0.)),\n",
        "}\n",
        "\n",
        "def variational_model(variational_params):\n",
        "    \"\"\"\n",
        "    Variational model q(w; eta)\n",
        "    arg: variational parameters \"eta\"\n",
        "    return: w ~ q(w; eta)\n",
        "    \"\"\"\n",
        "#     w0_q = torchdist.Normal(\n",
        "#         variational_params[\"w0_loc\"],\n",
        "#         torch.exp(variational_params[\"w0_scale_log\"]),\n",
        "#     )\n",
        "    \n",
        "    w1_q = torchdist.Normal(\n",
        "        variational_params[\"w1_loc\"],\n",
        "        torch.exp(variational_params[\"w1_scale_log\"]),\n",
        "    )\n",
        "    \n",
        "    w2_q = torchdist.Normal(\n",
        "        variational_params[\"w2_loc\"],\n",
        "        torch.exp(variational_params[\"w2_scale_log\"]),\n",
        "    )\n",
        "    \n",
        "    return w1_q, w2_q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2wPk26psMZ_w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def kl_divergence(variational_params, x, y):\n",
        "    w1_q, w2_q = variational_model(variational_params)\n",
        "    \n",
        "    w1_sample = w1_q.sample()    \n",
        "    w2_sample = w2_q.sample()\n",
        "    \n",
        "    log_joint_prob_value = log_joint_prob(w1_sample, w2_sample, x, y)\n",
        "    log_variational_prob_value = (\n",
        "        w1_q.log_prob(w1_sample) +\n",
        "        w2_q.log_prob(w2_sample)\n",
        "    )\n",
        "    \n",
        "    return log_variational_prob_value - log_joint_prob_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vxUWT2-uMkyM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "70d7000c-3427-4d8c-f6f0-a4efa6f5ffba"
      },
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(params=variational_params.values(), lr=1e-8)\n",
        "\n",
        "for i in range(9000):\n",
        "    optimizer.zero_grad()\n",
        "    loss_value =kl_divergence(variational_params, x_train, y_train)\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (i+1) % 300 == 0 or (i==0):\n",
        "        print(loss_value.detach().numpy())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "90.803825\n",
            "41.785004\n",
            "23.601\n",
            "39.208557\n",
            "19.011414\n",
            "181.76736\n",
            "37.377632\n",
            "21.076927\n",
            "45.070988\n",
            "34.490356\n",
            "35.906746\n",
            "63.28737\n",
            "28.20584\n",
            "93.55271\n",
            "18.816729\n",
            "24.749609\n",
            "31.443726\n",
            "26.173645\n",
            "107.47343\n",
            "48.67775\n",
            "48.917828\n",
            "20.044004\n",
            "19.199318\n",
            "20.335903\n",
            "19.125805\n",
            "45.805664\n",
            "19.173203\n",
            "22.83492\n",
            "28.226173\n",
            "46.18383\n",
            "21.605198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "m4ZuLxpJMtZ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "56543441-6c37-4544-e5d2-cb3c89f6c9ac"
      },
      "cell_type": "code",
      "source": [
        "for k, v in variational_params.items():\n",
        "    print(\"{} : {}\".format(k, v))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w1_loc : -2.330399865968502e-06\n",
            "w1_scale_log : 3.587514285641191e-08\n",
            "w2_loc : -2.8125245421506406e-07\n",
            "w2_scale_log : 1.359101929665485e-06\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}